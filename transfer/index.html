<html>
  <head>
    <title>Learning board value from action inputs only</title>
  </head>
  <body>
    <a href='https://okuvshynov.github.io/'>Home</a><br/>
    <h3>Learning board value from action inputs only</h3>
    <p>
    Here we look at self-play loop played for game Othello on 6x6 board. Similar to what Alpha Go Zero did years ago, it is continously playing against itself to collect training data, uses that data to train new model and evaluates the model.</p>

    <p>
    Model takes current board as an input and produces two outputs: probabilities for the next move and a single number representing expected score for the game if played from current position.</p>

    <p>
    Model definition is <a href='https://github.com/okuvshynov/rlscout/blob/247db981e4a3e6a7c7b5a8568de6f8cc4cd020ba/rlscout/model/action_value_model.py#L35'>here</a>. Residual tower is a shared part of the mode, with separate action and value parts attached to it.
    </p>

    <p>
    In current setup, we ignore value prediction entirely and not take value loss in training process at all - we only optimize for action. <a href='https://github.com/okuvshynov/rlscout/blob/247db981e4a3e6a7c7b5a8568de6f8cc4cd020ba/rlscout/train_loop.py#L31'>Here</a> we set the weight for the loss during training to be 0.
    </p>

    <img style='display: block; margin-left: auto; margin-right: auto;' src='action_only.png'/><br/>

    Still, as we can see here, loss for value output is declining as we train our model, despite value part of the model still being in randomly-initialized state and only shared part optimized for a different (but related) objective. It is not particularly surprising, but still fun to observe.
    If we wait longer though, we'll see how the loss started going up again, likely as shared part of the model gets more specialized in action part rather than learning generic facts about the game and board.

    <img style='display: block; margin-left: auto; margin-right: auto;' src='overfit.png'/><br/>
